{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.callbacks import ProgbarLogger\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import six, csv\n",
    "from collections import OrderedDict, Iterable\n",
    "from keras import optimizers\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import SpatialDropout1D\n",
    "from sklearn.metrics import precision_score "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model (Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the datasets\n",
    "sc_x_train = np.load('sc_x_train_sm3.npy')\n",
    "sc_y_train = np.load('sc_y_train_sm3.npy')\n",
    "sc_x_test = np.load('sc_x_test.npy')\n",
    "sc_y_test = np.load('sc_y_test.npy')\n",
    "y_test = np.load('y_test.npy')\n",
    "y_train = np.load('y_train.npy')\n",
    "y_test_class = np.load('y_test_class.npy', allow_pickle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format dataset for model input\n",
    "sc_y_train = sc_y_train.flatten()\n",
    "print(sc_y_train.shape)\n",
    "sc_y_test = sc_y_test.flatten()\n",
    "print(sc_y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric def RMSE\n",
    "def rmse(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving results per batch\n",
    "class NBatchCSVLogger(Callback):\n",
    "    \"\"\"Callback that streams every batch results to a csv file.\n",
    "    \"\"\"\n",
    "    def __init__(self, filename, separator=',', append=False):\n",
    "        self.sep = separator\n",
    "        self.filename = filename\n",
    "        self.append = append\n",
    "        self.writer = None\n",
    "        self.keys = None\n",
    "        self.append_header = True\n",
    "        self.file_flags = 'b' if six.PY2 and os.name == 'nt' else ''\n",
    "        super(NBatchCSVLogger, self).__init__()\n",
    "    def on_train_begin(self, logs=None):\n",
    "        if self.append:\n",
    "            if os.path.exists(self.filename):\n",
    "                with open(self.filename, 'r' + self.file_flags) as f:\n",
    "                    self.append_header = not bool(len(f.readline()))\n",
    "            self.csv_file = open(self.filename, 'a' + self.file_flags)\n",
    "        else:\n",
    "            self.csv_file = open(self.filename, 'w' + self.file_flags)\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        logs = logs or {}\n",
    "        def handle_value(k):\n",
    "            is_zero_dim_ndarray = isinstance(k, np.ndarray) and k.ndim == 0\n",
    "            if isinstance(k, six.string_types):\n",
    "                return k\n",
    "            elif isinstance(k, Iterable) and not is_zero_dim_ndarray:\n",
    "                return '\"[%s]\"' % (', '.join(map(str, k)))\n",
    "            else:\n",
    "                return k\n",
    "        if self.keys is None:\n",
    "            self.keys = sorted(logs.keys())\n",
    "        if self.model.stop_training:\n",
    "            logs = dict([(k, logs[k]) if k in logs else (k, 'NA') for k in self.keys])\n",
    "        if not self.writer:\n",
    "            class CustomDialect(csv.excel):\n",
    "                delimiter = self.sep\n",
    "            self.writer = csv.DictWriter(self.csv_file,\n",
    "                                         fieldnames=['batch'] + self.keys, dialect=CustomDialect)\n",
    "            if self.append_header:\n",
    "                self.writer.writeheader()\n",
    "        row_dict = OrderedDict({'batch': batch})\n",
    "        row_dict.update((key, handle_value(logs[key])) for key in self.keys)\n",
    "        self.writer.writerow(row_dict)\n",
    "        self.csv_file.flush()\n",
    "    def on_train_end(self, logs=None):\n",
    "        self.csv_file.close()\n",
    "        self.writer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and evaluate a model\n",
    "def evaluate_model(trainX, trainy, testX, testy):\n",
    "    verbose, epochs, batch_size = 0, 100, 100\n",
    "    n_timesteps, n_features = trainX.shape[1], trainX.shape[2]\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=20, kernel_size=2, activation='relu', input_shape=(n_timesteps,n_features), kernel_initializer = 'he_normal'))\n",
    "    model.add(SpatialDropout1D(0.5))\n",
    "    model.add(Conv1D(filters=10, kernel_size=2, activation='relu', kernel_initializer = 'he_normal'))\n",
    "    model.add(MaxPooling1D(pool_size=3))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(20, activation='relu', kernel_initializer = 'he_normal'))\n",
    "    model.add(Dense(1))\n",
    "    #opt = optimizers.Adam(learning_rate=0.5)\n",
    "    model.compile(loss='mse', metrics =[rmse], optimizer='adam')\n",
    "    \n",
    "    # fit network\n",
    "    csv_logger = CSVLogger('training_data.log')\n",
    "    history = model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, validation_data = (testX, testy), callbacks=[csv_logger], verbose=verbose)\n",
    "    out_batch = NBatchCSVLogger(\"batch_logs.csv\", separator=',', append=False)\n",
    "    #historyb = model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, callbacks=[out_batch], verbose=verbose)\n",
    "    \n",
    "    # save everyhting for later\n",
    "    wfname = \"weights.hdf5\"\n",
    "    fname = \"model.hdf5\"\n",
    "    model.save_weights(wfname,overwrite = True)\n",
    "    model.save(fname, overwrite = True)\n",
    "    \n",
    "    # evaluate model\n",
    "    mse = model.evaluate(testX, testy, batch_size=batch_size, verbose=verbose)\n",
    "\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evalutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_results(scores):\n",
    "    print(scores)\n",
    "    m, s = mean(scores), std(scores)\n",
    "    print('RMSE:mean',m,'std+/-',s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running evaluation for a number of times to summarize the results\n",
    "scores = list()\n",
    "def run_experiment(repeats=10):\n",
    "    # repeat experiment\n",
    "    for r in range(repeats):\n",
    "        score = evaluate_model(sc_x_train, sc_y_train, sc_x_test, sc_y_test)\n",
    "        #ensure that the metrics is rmse\n",
    "        print('>#%d: %.3f' % (r+1, score[1]))\n",
    "        scores.append(score[1])\n",
    "\n",
    "run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_scores = np.reshape(scores, (len(scores),1))\n",
    "re_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescale the results\n",
    "# inverse transform model predictions\n",
    "\n",
    "scaler_out = MinMaxScaler()\n",
    "scaler_out.fit(y_train)\n",
    "rmse1 = scaler_out.inverse_transform(re_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize results\n",
    "summarize_results(rmse1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting last fitting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading last repetition of model\n",
    "fname = \"model.hdf5\"\n",
    "model = load_model(fname, custom_objects={\"rmse\":rmse})\n",
    "\n",
    "# Loading model training history \n",
    "history=pd.read_csv(\"training_data.log\")\n",
    "historyb=pd.read_csv(\"batch_logs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historyb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_layer = model.layers[0]\n",
    "plt.imshow(top_layer.get_weights()[0][:, :, 0].squeeze(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check runs per epoch (loss)\n",
    "\n",
    "loss_train = history['loss']\n",
    "loss_val = history['val_loss']\n",
    "epochs = range(1, len(history['epoch']) +1)\n",
    "plt.plot(epochs, loss_train, 'g', label='Training loss')\n",
    "plt.plot(epochs, loss_val, 'b', label='Validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check runs per epoch (rmse)\n",
    "\n",
    "rmse_train = history['rmse']\n",
    "rmse_val = history['val_rmse']\n",
    "epochs = range(1,len(history['epoch']) +1)\n",
    "plt.plot(epochs, rmse_train, 'g', label='Training RMSE')\n",
    "plt.plot(epochs, rmse_val, 'b', label='Validation RMSE')\n",
    "plt.title('Training and Validation RMSE')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('RMSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(historyb['batch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check runs per batch (rmse)\n",
    "\n",
    "rmse_train = historyb['rmse']\n",
    "batch = range(0,len(historyb['batch']))\n",
    "plt.plot(batch, rmse_train, 'g', label='Training RMSE')\n",
    "plt.title('Training RMSE')\n",
    "plt.xlabel('Batches (32)')\n",
    "plt.ylabel('RMSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking output of the model\n",
    "model_prediction = model.predict(sc_x_test, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverse transform model predictions\n",
    "\n",
    "scaler_out = MinMaxScaler()\n",
    "scaler_out.fit(y_train)\n",
    "report = scaler_out.inverse_transform(model_prediction)\n",
    "print(report[0:2,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting predictions\n",
    "\n",
    "target = list(y_test.flatten())\n",
    "prediction = list(report.flatten())\n",
    "targe = target[0:21]\n",
    "pred =prediction[0:21]\n",
    "patterns = range(0,21)\n",
    "plt.plot(patterns, pred, 'g', label='Model predictions')\n",
    "plt.plot(patterns, targe, 'b', label='Target values')\n",
    "plt.xticks(patterns)\n",
    "plt.title('Target vs Predicted values')\n",
    "plt.xlabel('Patterns')\n",
    "plt.ylabel('Values')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare to classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert prediction set to classes\n",
    "\n",
    "pred_class = np.empty((report.shape[0],report.shape[1]), dtype = 'O')\n",
    "\n",
    "maxi = max(report[:,0])\n",
    "\n",
    "for c in range(pred_class.shape[0]):\n",
    "    if 0 <= report[c,0] <= 12:\n",
    "        pred_class[c,0] = \"good\"\n",
    "    if 12 < report[c,0] <= 35:\n",
    "        pred_class[c,0] = \"moderate\"\n",
    "    if 35 < report[c,0] <= 55:\n",
    "        pred_class[c,0] = \"unhealthy for sensitive groups\"\n",
    "    if 55 < report[c,0] <= 150:\n",
    "        pred_class[c,0] = \"unhealthy\"\n",
    "    if 150 < report[c,0] <= 250:\n",
    "        pred_class[c,0] = \"very unhealthy\"\n",
    "    if 250 < report[c,0] <= maxi:\n",
    "        pred_class[c,0] = \"hazardous\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = list(pred_class[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = precision_score(y_test_class, predict, labels=['good', 'hazardous', 'moderate', 'unhealthy', 'unhealthy for sensitive groups', 'very unhealthy'], average='micro')\n",
    "print('> %.3f' % (pre * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize exreme cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extreme cases from 150 onwards\n",
    "x_test_ex = np.reshape(sc_x_test, (sc_x_test.shape[0],sc_x_test.shape[1]*sc_x_test.shape[2]))\n",
    "print(x_test_ex.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checking = np.concatenate((x_test_ex, y_test), axis = 1)\n",
    "extreme = checking[:,105:106]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checking2 = checking[extreme[:,0]>150]\n",
    "print(checking2.shape)\n",
    "checking3 = checking2[:,0:105]\n",
    "print(checking3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checking4 = np.reshape(checking3, (checking3.shape[0],sc_x_test.shape[1], sc_x_test.shape[2]))\n",
    "checking4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extreme_predict = model.predict(checking4, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report2 = scaler_out.inverse_transform(extreme_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checking5 = checking2[:,105:106]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting extreme predictions\n",
    "\n",
    "target = list(checking5.flatten())\n",
    "prediction = list(report2.flatten())\n",
    "targe = target\n",
    "pred =prediction\n",
    "patterns = range(0,136)\n",
    "plt.plot(patterns, pred, 'g', label='Model predictions')\n",
    "plt.plot(patterns, targe, 'b', label='Target values')\n",
    "plt.xticks(patterns)\n",
    "plt.title('Target vs Predicted values')\n",
    "plt.xlabel('Patterns')\n",
    "plt.ylabel('Values')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
