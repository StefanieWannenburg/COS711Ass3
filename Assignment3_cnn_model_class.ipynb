{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.callbacks import CSVLogger\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import six, csv\n",
    "from collections import OrderedDict, Iterable\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import SpatialDropout1D\n",
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model (Classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the datasets\n",
    "sc_x_train = np.load('sc_x_train_class.npy')\n",
    "sc_y_train = np.load('sc_y_train_class.npy')\n",
    "sc_x_test = np.load('sc_x_test.npy')\n",
    "sc_y_test = np.load('sc_y_test_class.npy')\n",
    "y_test = np.load('y_test_class.npy', allow_pickle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sc_x_train.shape)\n",
    "print(sc_y_train.shape)\n",
    "print(sc_x_test.shape)\n",
    "print(sc_y_test.shape)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(trainX, trainy, testX, testy, rawy):\n",
    "\n",
    "    # define model architecture\n",
    "    verbose, epochs, batch_size = 0, 100, 100\n",
    "    n_timesteps, n_features = trainX.shape[1], trainX.shape[2]\n",
    "    model_class = Sequential()\n",
    "    model_class.add(Conv1D(filters=20, kernel_size=2, activation='relu', input_shape=(n_timesteps,n_features), kernel_initializer = 'he_normal'))\n",
    "    model_class.add(SpatialDropout1D(0.5))\n",
    "    model_class.add(Conv1D(filters=10, kernel_size=2, activation='relu', kernel_initializer = 'he_normal'))\n",
    "    model_class.add(MaxPooling1D(pool_size=3))\n",
    "    model_class.add(Flatten())\n",
    "    model_class.add(Dropout(0.5))\n",
    "    model_class.add(Dense(20, activation='relu', kernel_initializer = 'he_normal'))\n",
    "    model_class.add(Dense(6, activation = 'softmax'))\n",
    "    model_class.compile(loss='categorical_crossentropy', metrics =['accuracy'], optimizer='adam')\n",
    "    \n",
    "    # fit network\n",
    "    csv_logger = CSVLogger('training_data_class.log')\n",
    "    history = model_class.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, validation_data = (testX, testy), callbacks=[csv_logger], verbose=verbose)\n",
    "    \n",
    "    # save everything for later\n",
    "    wfname = \"weights_class.hdf5\"\n",
    "    fname = \"model_class.hdf5\"\n",
    "    model_class.save_weights(wfname,overwrite = True)\n",
    "    model_class.save(fname, overwrite = True)\n",
    "    \n",
    "    # evaluate model\n",
    "    loss , _ = model_class.evaluate(testX, testy, batch_size=batch_size, verbose=verbose)\n",
    "    print('> %.3f' % (loss))\n",
    "    \n",
    "    #predict model\n",
    "    prediction = model_class.predict(testX, verbose = 0)\n",
    "    report = np.argmax(prediction, axis = -1)\n",
    "        # inverse transform model predictions\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(rawy)\n",
    "    report1 = encoder.inverse_transform(report)\n",
    "    pre = precision_score(rawy, report1, labels=['good', 'hazardous', 'moderate', 'unhealthy', 'unhealthy for sensitive groups', 'very unhealthy'], average='micro')\n",
    "    print('> %.3f' % (pre * 100.0))\n",
    "    \n",
    "    return loss, pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run simulations\n",
    "scores, error = list(), list()\n",
    "\n",
    "def run_experiment(repeats=10):\n",
    "    # repeat experiment\n",
    "    for r in range(repeats):\n",
    "        loss, pre = evaluate_model(sc_x_train, sc_y_train, sc_x_test, sc_y_test, y_test)\n",
    "        scores.append(pre)\n",
    "        error.append(loss)\n",
    "\n",
    "run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Presenting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading last repetition of model\n",
    "fname = \"model_class.hdf5\"\n",
    "model_class = load_model(fname)\n",
    "\n",
    "# Loading model training history \n",
    "history=pd.read_csv(\"training_data_class.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss\n",
    "plt.subplot(211)\n",
    "plt.title('Cross Entropy Loss')\n",
    "plt.plot(history['loss'], color='g', label='Training loss')\n",
    "plt.plot(history['val_loss'], color='b', label='Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot accuracy\n",
    "plt.subplot(212)\n",
    "plt.title('Classification Accuracy')\n",
    "plt.plot(history['accuracy'], color='g', label='Training accuracy')\n",
    "plt.plot(history['val_accuracy'], color='b', label='Validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize model performance\n",
    "def summarize_performance(scores, loss):\n",
    "    # print summary\n",
    "    print('mean=%.3f std=%.3f, n=%d' % (mean(loss), std(loss), len(loss)))\n",
    "    print('mean=%.3f std=%.3f, n=%d' % (mean(scores)*100, std(scores)*100, len(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize estimated loss\n",
    "summarize_performance(scores = scores ,loss = error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading last repetition of model\n",
    "fname = \"model_class.hdf5\"\n",
    "model_class = load_model(fname)\n",
    "\n",
    "# Loading model training history \n",
    "history=pd.read_csv(\"training_data_class.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_class.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_layer = model_class.layers[0]\n",
    "plt.imshow(top_layer.get_weights()[0][:, :, 0].squeeze(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking output of the model\n",
    "model_prediction = model_class.predict(sc_x_test, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "som = np.sum(model_prediction[0,:])\n",
    "som"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert back to numbers\n",
    "\n",
    "report = np.argmax(model_prediction, axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverse transform model predictions\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_test)\n",
    "report2 = encoder.inverse_transform(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting predictions\n",
    "\n",
    "target = y_test\n",
    "prediction = report2\n",
    "targe = target[0:21]\n",
    "pred =prediction[0:21]\n",
    "patterns = range(0,21)\n",
    "plt.plot(patterns, pred, 'g', label='Model predictions')\n",
    "plt.plot(patterns, targe, 'b', label='Target values')\n",
    "plt.xticks(patterns)\n",
    "plt.title('Target vs Predicted values')\n",
    "plt.xlabel('Patterns')\n",
    "plt.ylabel('Classes')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "precision = precision_score(target, prediction, labels=['good', 'hazardous', 'moderate', 'unhealthy', 'unhealthy for sensitive groups', 'very unhealthy'], average='micro')\n",
    "print('Precision: %.3f' % precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "for a in range(0,len(y_test)):\n",
    "    if y_test[a] == report2[a]:\n",
    "        correct  = correct +1\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "accuracy_test = correct/len(y_test)*100\n",
    "accuracy_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extreme cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extreme cases from 150 onwards\n",
    "x_test_ex = np.reshape(sc_x_test, (sc_x_test.shape[0],sc_x_test.shape[1]*sc_x_test.shape[2]))\n",
    "print(x_test_ex.shape)\n",
    "y_test_ex = np.reshape(y_test, (y_test.shape[0],1))\n",
    "print(y_test_ex.shape)\n",
    "y_test_ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checking = np.concatenate((x_test_ex, y_test_ex), axis = 1)\n",
    "print(checking.shape)\n",
    "extreme = checking[:,105:106]\n",
    "extreme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checking2 = checking[(extreme[:,0]=='hazardous')|(extreme[:,0]=='very unhealthy')]\n",
    "print(checking2.shape)\n",
    "checking3 = checking2[:,0:105]\n",
    "print(checking3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checking4 = np.reshape(checking3, (checking3.shape[0],sc_x_test.shape[1], sc_x_test.shape[2]))\n",
    "checking4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extreme_predict = model_class.predict(checking4, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report3 = np.argmax(extreme_predict, axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report4 = encoder.inverse_transform(report3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checking5 = checking2[:,105:106]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting extreme predictions\n",
    "\n",
    "target = list(checking5.flatten())\n",
    "prediction = list(report2.flatten())\n",
    "targe = target[0:21]\n",
    "pred =prediction[0:21]\n",
    "patterns = range(0,21)\n",
    "plt.plot(patterns, pred, 'g', label='Model predictions')\n",
    "plt.plot(patterns, targe, 'b', label='Target values')\n",
    "plt.xticks(patterns)\n",
    "plt.title('Target vs Predicted values')\n",
    "plt.xlabel('Patterns')\n",
    "plt.ylabel('Classes')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
